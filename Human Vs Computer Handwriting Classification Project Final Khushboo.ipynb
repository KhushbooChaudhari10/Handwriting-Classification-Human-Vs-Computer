{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72929ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage import exposure \n",
    "import shutil \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93211582",
   "metadata": {},
   "source": [
    "### GENERATE COMPUTER DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a709764",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_all_font_files(folder_path):\n",
    "    return [f for f in os.listdir(folder_path) if f.lower().endswith(('.ttf', '.otf'))]\n",
    "\n",
    "def get_words_from_list():\n",
    "    return [\n",
    "        \"Lorem\", \"ipsum\", \"dolor\", \"sit\", \"amet\", \"consectetur\", \"adipiscing\", \"elit\", \"sed\", \"do\", \"eiusmod\", \"tempor\", \"incididunt\",\n",
    "        \"ut\", \"labore\", \"et\", \"dolore\", \"magna\", \"aliqua\", \"ut\", \"enim\", \"ad\", \"minim\", \"veniam\", \"quis\", \"nostrud\", \"exercitation\", \"ullamco\",\n",
    "        \"laboris\", \"nisi\", \"ut\", \"aliquip\", \"ex\", \"ea\", \"commodo\", \"consequat\", \"duis\", \"aute\", \"irure\", \"dolor\", \"in\", \"reprehenderit\", \"in\",\n",
    "        \"voluptate\", \"velit\", \"esse\", \"cillum\", \"dolore\", \"eu\", \"fugiat\", \"nulla\", \"pariatur\", \"excepteur\", \"sint\", \"occaecat\", \"cupidatat\",\n",
    "        \"non\", \"proident\", \"sunt\", \"in\", \"culpa\", \"qui\", \"officia\", \"deserunt\", \"mollit\", \"anim\", \"id\", \"est\", \"laborum\", \"The\", \"quick\",\n",
    "        \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog.\", \"Pack\", \"my\", \"box\", \"with\", \"five\", \"dozen\", \"liquor\", \"jugs.\", \"Random\",\n",
    "        \"text\", \"generator\", \"is\", \"a\", \"handy\", \"tool\", \"for\", \"generating\", \"lorem\", \"ipsum-like\", \"text\", \"or\", \"other\", \"random\", \"sequences.\",\n",
    "        \"It\", \"can\", \"be\", \"used\", \"in\", \"design\", \"mockups,\", \"websites,\", \"and\", \"other\", \"projects\", \"to\", \"fill\", \"space\", \"and\", \"create\",\n",
    "        \"a\", \"sense\", \"of\", \"how\", \"the\", \"final\", \"product\", \"will\", \"look.\", \"Python\", \"is\", \"a\", \"high-level\", \"programming\", \"language\",\n",
    "        \"known\", \"for\", \"its\", \"readability\", \"and\", \"versatility.\", \"It\", \"is\", \"widely\", \"used\", \"in\", \"web\", \"development,\", \"data\", \"science,\",\n",
    "        \"artificial\", \"intelligence,\", \"and\", \"more.\", \"In\", \"a\", \"galaxy\", \"far,\", \"far\", \"away,\", \"there\", \"is\", \"a\", \"planet\", \"inhabited\",\n",
    "        \"by\", \"curious\", \"aliens\", \"who\", \"communicate\", \"through\", \"a\", \"complex\", \"system\", \"of\", \"colors\", \"and\", \"shapes.\", \"Chasing\", \"the\",\n",
    "        \"sunset,\", \"the\", \"adventurer\", \"climbed\", \"the\", \"mountain,\", \"eager\", \"to\", \"discover\", \"what\", \"lay\", \"beyond\", \"the\", \"horizon.\",\n",
    "        \"Coffee\", \"is\", \"the\", \"elixir\", \"of\", \"productivity\", \"for\", \"many,\", \"providing\", \"the\", \"energy\", \"needed\", \"to\", \"tackle\", \"the\",\n",
    "        \"day's\", \"challenges.\", \"The\", \"melody\", \"of\", \"the\", \"piano\", \"echoed\", \"through\", \"the\", \"empty\", \"hall,\", \"creating\", \"an\", \"atmosphere\",\n",
    "        \"of\", \"melancholy\", \"and\", \"nostalgia.\", \"In\", \"the\", \"world\", \"of\", \"possibilities,\", \"creativity\", \"knows\", \"no\", \"bounds.\", \"Imagination\",\n",
    "        \"is\", \"the\", \"key\", \"to\", \"unlocking\", \"new\", \"and\", \"exciting\", \"ideas.\", \"A\", \"journey\", \"of\", \"a\", \"thousand\", \"miles\", \"begins\", \"with\",\n",
    "        \"a\", \"single\", \"step.\", \"Life\", \"is\", \"a\", \"series\", \"of\", \"natural\", \"and\", \"spontaneous\", \"changes.\", \"Don't\", \"resist\", \"them;\", \"that\",\n",
    "        \"only\", \"creates\", \"sorrow.\", \"Let\", \"reality\", \"be\", \"reality.\", \"Let\", \"things\", \"flow\", \"naturally\", \"forward\", \"in\", \"whatever\", \"way\",\n",
    "        \"they\", \"like.\", \"The\", \"only\", \"limit\", \"to\", \"our\", \"realization\", \"of\", \"tomorrow\", \"will\", \"be\", \"our\", \"doubts\", \"of\", \"today.\", \"It\",\n",
    "        \"does\", \"not\", \"matter\", \"how\", \"slowly\", \"you\", \"go\", \"as\", \"long\", \"as\", \"you\", \"do\", \"not\", \"stop.\", \"Success\", \"is\", \"not\", \"final,\",\n",
    "        \"failure\", \"is\", \"not\", \"fatal:\", \"It\", \"is\", \"the\", \"courage\", \"to\", \"continue\", \"that\", \"counts.\", \"Your\", \"work\", \"is\", \"going\", \"to\",\n",
    "        \"fill\", \"a\", \"large\", \"part\", \"of\", \"your\", \"life,\", \"and\", \"the\", \"only\", \"way\", \"to\", \"be\", \"truly\", \"satisfied\", \"is\", \"to\", \"do\", \"what\",\n",
    "        \"you\", \"believe\", \"is\", \"great\", \"work.\", \"And\", \"the\", \"only\", \"way\", \"to\", \"do\", \"great\", \"work\", \"is\", \"to\", \"love\", \"what\", \"you\", \"do.\",\n",
    "        \"If\", \"you\", \"haven't\", \"found\", \"it\", \"yet,\", \"keep\", \"looking.\", \"Don't\", \"settle.\", \"As\", \"with\", \"all\", \"matters\", \"of\", \"the\", \"heart,\",\n",
    "        \"you'll\", \"know\", \"when\", \"you\", \"find\", \"it.\", \"The\", \"future\", \"belongs\", \"to\", \"those\", \"who\", \"believe\", \"in\", \"the\", \"beauty\", \"of\",\n",
    "        \"their\", \"dreams.\", \"Don't\", \"watch\", \"the\", \"clock;\", \"do\", \"what\", \"it\", \"does.\", \"Keep\", \"going.\", \"You\", \"are\", \"never\", \"too\", \"old\",\n",
    "        \"to\", \"set\", \"another\", \"goal\", \"or\", \"to\", \"dream\", \"a\", \"new\", \"dream.\", \"The\", \"only\", \"way\", \"to\", \"do\", \"great\", \"work\", \"is\",\n",
    "        # ... (add more words as needed)\n",
    "    ]\n",
    "\n",
    "# Get the path to the fonts folder\n",
    "fonts_folder = \"D:/hp/Desktop/msc ds/sem3/New DL Project/DL Words/FONTS\"\n",
    "\n",
    "# Get a list of all the font files in the folder\n",
    "font_files = get_all_font_files(fonts_folder)\n",
    "\n",
    "# Create the \"comp_words\" folder if it doesn't exist\n",
    "output_folder = \"D:/hp/Desktop/msc ds/sem3/New DL Project/DL Words/comp_words\"\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Generate 3000 images\n",
    "for i in range(3000):\n",
    "    # Select a random font from the fonts folder\n",
    "    font_path = os.path.join(fonts_folder, random.choice(font_files))\n",
    "\n",
    "    # Create a PIL ImageFont object with the selected font and random font size\n",
    "    font = ImageFont.truetype(font_path, random.randint(16, 32))\n",
    "    \n",
    "    # Calculate the text dimensions for the selected word and font\n",
    "    text = random.choice(get_words_from_list())\n",
    "    text_width, text_height = font.getsize_multiline(text)\n",
    "    \n",
    "    # Adjust the image size based on the calculated text dimensions\n",
    "    image_width = min(text_width + 10, 256)\n",
    "    image_height = min(text_height + 10, 256)\n",
    "\n",
    "    # Create a blank image with the adjusted dimensions\n",
    "    image = Image.new(\"RGB\", (image_width, image_height), (255, 255, 255))\n",
    "\n",
    "    # Draw the selected text on the image\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    draw.multiline_text((10, 10), text, font=font, fill=(0, 0, 0))\n",
    "\n",
    "    # Save the image to the \"comp_words\" folder\n",
    "    image_filename = os.path.join(output_folder, f\"image_{i+1}.jpg\")\n",
    "    image.save(image_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312a44aa",
   "metadata": {},
   "source": [
    "### PREPROCESS COMPUTER DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fed2e44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Input and output folders\n",
    "input_folder = \"D:/hp/Desktop/msc ds/sem3/New DL Project/DL Words/comp_words\"\n",
    "output_folder = \"D:/hp/Desktop/msc ds/sem3/New DL Project/DL Words/preprocessed_comp_words\"\n",
    "\n",
    "# Create output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# List all files in the input folder\n",
    "image_files = [f for f in os.listdir(input_folder) if os.path.isfile(os.path.join(input_folder, f))]\n",
    "\n",
    "# Loop through each image file\n",
    "for file_name in image_files:\n",
    "    # Read the image\n",
    "    image_path = os.path.join(input_folder, file_name)\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # 1. Resize images to a consistent size\n",
    "    target_size = (224, 224)  # Adjust the target size as needed\n",
    "    resized_img = cv2.resize(img, target_size)\n",
    "\n",
    "    # 2. Normalize pixel values (scale them to the range [0, 1])\n",
    "    normalized_img = resized_img / 255.0\n",
    "\n",
    "    # 3. Histogram Equalization\n",
    "    equalized_img = exposure.equalize_hist(normalized_img)\n",
    "\n",
    "    # 4. Standardization\n",
    "    mean = np.mean(normalized_img)\n",
    "    std = np.std(normalized_img)\n",
    "    standardized_img = (normalized_img - mean) / std\n",
    "\n",
    "    # 5. Rescaling to a Different Range\n",
    "    rescaled_img = (normalized_img - 0.5) / 0.5\n",
    "\n",
    "    # Choose the processed image based on your experimentation and requirements\n",
    "    preprocessed_img = normalized_img  # Adjust as needed\n",
    "\n",
    "    # Save the preprocessed image to the output folder\n",
    "    output_path = os.path.join(output_folder, f\"preprocessed_{file_name}\")\n",
    "    cv2.imwrite(output_path, (preprocessed_img * 255).astype(np.uint8))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3f45de",
   "metadata": {},
   "source": [
    "### PREPROCESS HANDWRITTEN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13e93617",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_23696\\3903914358.py:30: RuntimeWarning: invalid value encountered in true_divide\n",
      "  standardized_img = (normalized_img - mean) / std\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Input and output folders\n",
    "input_folder = \"D:/hp/Desktop/msc ds/sem3/New DL Project/DL Words/hand_words\"\n",
    "output_folder = \"D:/hp/Desktop/msc ds/sem3/New DL Project/DL Words/preprocessed_hand_words\"\n",
    "\n",
    "# Create output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# List all files in the input folder\n",
    "image_files = [f for f in os.listdir(input_folder) if os.path.isfile(os.path.join(input_folder, f))]\n",
    "\n",
    "# Loop through each image file\n",
    "for file_name in image_files:\n",
    "    # Read the image\n",
    "    image_path = os.path.join(input_folder, file_name)\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # 1. Resize images to a consistent size\n",
    "    target_size = (224, 224)  # Adjust the target size as needed\n",
    "    resized_img = cv2.resize(img, target_size)\n",
    "\n",
    "    # 2. Normalize pixel values (scale them to the range [0, 1])\n",
    "    normalized_img = resized_img / 255.0\n",
    "\n",
    "    # 3. Histogram Equalization\n",
    "    equalized_img = exposure.equalize_hist(normalized_img)\n",
    "\n",
    "    # 4. Standardization\n",
    "    mean = np.mean(normalized_img)\n",
    "    std = np.std(normalized_img)\n",
    "    standardized_img = (normalized_img - mean) / std\n",
    "\n",
    "    # 5. Rescaling to a Different Range\n",
    "    rescaled_img = (normalized_img - 0.5) / 0.5\n",
    "\n",
    "    # Choose the processed image based on your experimentation and requirements\n",
    "    preprocessed_img = normalized_img  # Adjust as needed\n",
    "\n",
    "    # Save the preprocessed image to the output folder\n",
    "    output_path = os.path.join(output_folder, f\"preprocessed_{file_name}\")\n",
    "    cv2.imwrite(output_path, (preprocessed_img * 255).astype(np.uint8))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88247cc9",
   "metadata": {},
   "source": [
    "### SPLITING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24e2ac85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the source directories for labeled data\n",
    "data_dir = \"D:/hp/Desktop/msc ds/sem3/New DL Project/DL Words/labeled_data\"\n",
    "preprocessed_hand_words = \"D:/hp/Desktop/msc ds/sem3/New DL Project/DL Words/preprocessed_hand_words\"\n",
    "human_handwriting_dir = os.path.join(preprocessed_hand_words)\n",
    "preprocessed_comp_words = \"D:/hp/Desktop/msc ds/sem3/New DL Project/DL Words/preprocessed_comp_words\"\n",
    "computer_writing_dir = os.path.join(preprocessed_comp_words)\n",
    "\n",
    "# Define the destination directories for the splits\n",
    "base_dir = \"D:/hp/Desktop/msc ds/sem3/New DL Project/DL Words/data_split\"\n",
    "train_dir = os.path.join(base_dir, \"train\")\n",
    "valid_dir = os.path.join(base_dir, \"valid\")\n",
    "test_dir = os.path.join(base_dir, \"test\")\n",
    "\n",
    "# Create destination directories if they don't exist\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(valid_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# List images in each class folder\n",
    "human_handwriting_images = os.listdir(human_handwriting_dir)\n",
    "computer_writing_images = os.listdir(computer_writing_dir)\n",
    "\n",
    "# Shuffle the image lists to randomize the order\n",
    "random.shuffle(human_handwriting_images)\n",
    "random.shuffle(computer_writing_images)\n",
    "\n",
    "# Define split ratios\n",
    "train_ratio = 0.7\n",
    "valid_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "# Calculate the number of images for each split for both classes\n",
    "human_train_count = int(train_ratio * len(human_handwriting_images))\n",
    "human_valid_count = int(valid_ratio * len(human_handwriting_images))\n",
    "human_test_count = len(human_handwriting_images) - human_train_count - human_valid_count\n",
    "\n",
    "comp_train_count = int(train_ratio * len(computer_writing_images))\n",
    "comp_valid_count = int(valid_ratio * len(computer_writing_images))\n",
    "comp_test_count = len(computer_writing_images) - comp_train_count - comp_valid_count\n",
    "\n",
    "# Function to copy images to destination folder\n",
    "def copy_images(source_folder, dest_folder, image_list):\n",
    "    os.makedirs(dest_folder, exist_ok=True)\n",
    "    for image_filename in image_list:\n",
    "        source_path = os.path.join(source_folder, image_filename)\n",
    "        dest_path = os.path.join(dest_folder, image_filename)\n",
    "        shutil.copy(source_path, dest_path)\n",
    "\n",
    "# Copy images to the respective split directories for both classes\n",
    "copy_images(human_handwriting_dir, os.path.join(train_dir, \"human_handwriting\"), human_handwriting_images[:human_train_count])\n",
    "copy_images(human_handwriting_dir, os.path.join(valid_dir, \"human_handwriting\"), human_handwriting_images[human_train_count:human_train_count + human_valid_count])\n",
    "copy_images(human_handwriting_dir, os.path.join(test_dir, \"human_handwriting\"), human_handwriting_images[human_train_count + human_valid_count:])\n",
    "\n",
    "copy_images(computer_writing_dir, os.path.join(train_dir, \"computer_writing\"), computer_writing_images[:comp_train_count])\n",
    "copy_images(computer_writing_dir, os.path.join(valid_dir, \"computer_writing\"), computer_writing_images[comp_train_count:comp_train_count + comp_valid_count])\n",
    "copy_images(computer_writing_dir, os.path.join(test_dir, \"computer_writing\"), computer_writing_images[comp_train_count + comp_valid_count:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2474014",
   "metadata": {},
   "source": [
    "### MODEL BUILD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d58539ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 222, 222, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 111, 111, 32)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 109, 109, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 54, 54, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 52, 52, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (None, 26, 26, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 86528)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               11075712  \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11169089 (42.61 MB)\n",
      "Trainable params: 11169089 (42.61 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\nimport tensorflow as tf\\nfrom tensorflow.keras import layers, models\\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\\n\\n# Define the CNN model\\nmodel = models.Sequential()\\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 3)))\\nmodel.add(layers.MaxPooling2D((2, 2)))\\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\\nmodel.add(layers.MaxPooling2D((2, 2)))\\nmodel.add(layers.Flatten())\\nmodel.add(layers.Dense(64, activation='relu'))\\nmodel.add(layers.Dense(num_classes, activation='softmax'))\\n\\n# Compile the model\\nmodel.compile(optimizer='adam',\\n              loss='sparse_categorical_crossentropy',\\n              metrics=['accuracy'])\\n\\n\\n# Display the model summary\\nmodel.summary()\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Assuming your word images are grayscale and have shape (height, width, channels)\n",
    "input_shape = (224,224, 3)  # Update your_height and your_width accordingly\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',  # Binary classification\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define the CNN model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbba2a0",
   "metadata": {},
   "source": [
    "### MODEL FITTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95451a87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4198 images belonging to 2 classes.\n",
      "Found 899 images belonging to 2 classes.\n",
      "Found 901 images belonging to 2 classes.\n",
      "Epoch 1/5\n",
      "131/131 [==============================] - 537s 4s/step - loss: 0.2835 - accuracy: 0.8869 - val_loss: 0.1755 - val_accuracy: 0.9275\n",
      "Epoch 2/5\n",
      "131/131 [==============================] - 565s 4s/step - loss: 0.0625 - accuracy: 0.9789 - val_loss: 0.0636 - val_accuracy: 0.9743\n",
      "Epoch 3/5\n",
      "131/131 [==============================] - 529s 4s/step - loss: 0.0625 - accuracy: 0.9777 - val_loss: 0.0251 - val_accuracy: 0.9922\n",
      "Epoch 4/5\n",
      "131/131 [==============================] - 554s 4s/step - loss: 0.0403 - accuracy: 0.9870 - val_loss: 0.0412 - val_accuracy: 0.9922\n",
      "Epoch 5/5\n",
      "131/131 [==============================] - 558s 4s/step - loss: 0.0165 - accuracy: 0.9959 - val_loss: 0.0276 - val_accuracy: 0.9922\n",
      "29/29 [==============================] - 34s 1s/step - loss: 0.0564 - accuracy: 0.9834\n",
      "Test Accuracy: 0.9833518266677856\n",
      "Test Loss: 0.056405387818813324\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define your data directories\n",
    "train_dir = \"D:/hp/Desktop/msc ds/sem3/New DL Project/DL Words/data_split/train\"  # Path to training data\n",
    "valid_dir = \"D:/hp/Desktop/msc ds/sem3/New DL Project/DL Words/data_split/valid\"  # Path to validation data\n",
    "test_dir = \"D:/hp/Desktop/msc ds/sem3/New DL Project/DL Words/data_split/test\"   # Path to test data\n",
    "\n",
    "# Define model hyperparameters\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "image_size = (224, 224)\n",
    "num_classes = 2  # Two classes: Human Handwriting and Computer Writing\n",
    "\n",
    "# Create data generators for training, validation, and test sets\n",
    "train_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',  # 'binary' for 2 classes\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "valid_generator = valid_datagen.flow_from_directory(\n",
    "    valid_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',  # 'binary' for 2 classes\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',  # 'binary' for 2 classes\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=valid_generator.samples // batch_size\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "print(\"Test Loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69489863",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.save('Word_Prediction.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b3f46e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 331ms/step\n",
      "[[3.91619e-05]]\n",
      "Predicted class: Computer-generated\n"
     ]
    }
   ],
   "source": [
    "model = load_model('Word_Prediction.keras')\n",
    "\n",
    "# Path to the single image\n",
    "image_path = \"D:/hp/Desktop/msc ds/sem3/New DL Project/DL Words/check/word4.jpg\"\n",
    "\n",
    "# Load and preprocess the image for prediction\n",
    "img = image.load_img(image_path, target_size=(224, 224))\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "img_array /= 255.0  # Normalize pixel values\n",
    "\n",
    "# Make prediction\n",
    "predictions = model.predict(img_array)\n",
    "print (predictions)\n",
    "\n",
    "# Assuming binary classification, print the prediction result\n",
    "if predictions[0][0] > 0.5:\n",
    "    print(\"Predicted class: Human-generated\")\n",
    "else:\n",
    "    print(\"Predicted class: Computer-generated\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161c704f",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "42c3af64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 30s 1s/step\n",
      "Confusion Matrix:\n",
      "[[436  14]\n",
      " [  1 450]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98       450\n",
      "           1       0.97      1.00      0.98       451\n",
      "\n",
      "    accuracy                           0.98       901\n",
      "   macro avg       0.98      0.98      0.98       901\n",
      "weighted avg       0.98      0.98      0.98       901\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load your trained model\n",
    "  # Replace with the path to your saved model\n",
    "model = load_model('Word_Prediction.keras')\n",
    "\n",
    "# Load your test data\n",
    "# Replace 'X_test' and 'y_true' with your actual test data and true labels\n",
    "X_test = test_generator\n",
    "y_true = test_generator.classes\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)  # Assuming a threshold of 0.5 for binary classification\n",
    "\n",
    "# Create a confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Display the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Plot the confusion matrix using seaborn\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['Predicted Negative', 'Predicted Positive'],\n",
    "            yticklabels=['Actual Negative', 'Actual Positive'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Print a classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4582cd0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
